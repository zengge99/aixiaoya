import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import pickle
import sys
import os
import re
import random
import numpy as np
import glob
import string

# --- å…¨å±€æ ¸å¿ƒé…ç½® ---
NUM_THREADS = 4
BATCH_SIZE = 128
LR = 1e-3            # å­¦ä¹ ç‡
EPOCHS = 50          # è®­ç»ƒè½®æ•°
MAX_LEN = 300        # æœ€å¤§åºåˆ—é•¿åº¦
EMBED_DIM = 64      # å‘é‡ç»´åº¦
HIDDEN_DIM = 128     # éšè—å±‚ç»´åº¦

MODEL_PATH = "movie_model.pth"
VOCAB_PATH = "vocab.pkl"
# æ•°æ®æ–‡ä»¶åŒ¹é…æ¨¡å¼ (åŒ¹é… train_data.txt, train_data_2.txt ç­‰)
DATA_FILE_PATTERN = "train_data*.txt" 
SEED = 42            # å›ºå®šéšæœºç§å­

# --- é¢„æµ‹/è°ƒè¯•é…ç½® ---
DEBUG_MODE = False    # å¼€å¯è°ƒè¯•è¯¦æƒ…
THRESHOLD = 0.35     # åˆ¤å®šé˜ˆå€¼
SMOOTH_VAL = 0.1     # å¹³æ»‘æ•‘å›é˜ˆå€¼

# è®¾ç½®çº¿ç¨‹æ•°
torch.set_num_threads(NUM_THREADS)

# --- è¾…åŠ©å·¥å…·ç±» ---
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# --- ç§»æ¤çš„ JS é€»è¾‘å·¥å…·ç±» ---
class TextUtils:
    CN_NUMS = ["é›¶", "ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å"]

    @staticmethod
    def number2text(text):
        if not text: return text
        text = text.lstrip('0')
        if not text: return "é›¶" 

        try:
            num = int(text)
        except ValueError:
            return text

        if num <= 10:
            return TextUtils.CN_NUMS[num]
        elif num < 20:
            return "å" + TextUtils.CN_NUMS[num % 10]
        elif num % 10 == 0:
            return TextUtils.CN_NUMS[num // 10] + "å"
        else:
            return TextUtils.CN_NUMS[num // 10] + "å" + TextUtils.CN_NUMS[num % 10]

    @staticmethod
    def fix_name(path, ai_result):
        # åˆ¤æ–­æ˜¯å¦ä¸ºå…¨è‹±æ–‡ï¼ˆåŒ…å«å­—æ¯ã€ç©ºæ ¼ã€æ•°å­—ã€å¸¸è§æ ‡ç‚¹ï¼Œæ— ä¸­æ–‡å­—ç¬¦ï¼‰
        if ai_result and all(ord(c) < 128 for c in ai_result):
            return ai_result

        replace_patterns = [
            r'Season\s*(\d{1,2})',              
            r'SE(\d{1,2})',                     
            r'(?<![a-zA-Z])S(\d{1,2})(?![a-zA-Z])', 
            r'ç¬¬(\d{1,2})å­£'                    
        ]

        processed_result = ai_result
        replaced_flag = False 

        def replace_func(match):
            nonlocal replaced_flag
            replaced_flag = True
            num = match.group(1)
            cn_num = TextUtils.number2text(num)
            return f" ç¬¬{cn_num}å­£ " 

        for pattern in replace_patterns:
            if re.search(pattern, processed_result, re.IGNORECASE):
                processed_result = re.sub(pattern, replace_func, processed_result, flags=re.IGNORECASE)
        
        processed_result = re.sub(r'\s+', ' ', processed_result).strip()

        if replaced_flag:
            return processed_result

        path_search_patterns = [
            r'Season\s*(\d{1,2})',
            r'SE(\d{1,2})',
            r'ç¬¬(\d{1,2})å­£',
            r'(?<![A-Za-z])S(\d{1,2})'
        ]

        for pattern in path_search_patterns:
            match = re.search(pattern, path, re.IGNORECASE)
            if match:
                num = match.group(1)
                cn_num = TextUtils.number2text(num)
                suffix = f"ç¬¬{cn_num}å­£"
                if suffix not in processed_result:
                    return f"{processed_result} {suffix}"
                break 
        
        return processed_result

# --- æ¨¡å‹ç»“æ„ (CNN + BiGRU + Attention) ---
class Extractor(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):
        super().__init__()
        
        # 1. Embedding
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # 2. CNN (æå–å±€éƒ¨ n-gram ç‰¹å¾)
        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.norm1 = nn.LayerNorm(embed_dim) 
        
        # 3. BiGRU (å¢åŠ  num_layers å¹¶å¼€å¯ dropout æ—¶éœ€æ³¨æ„å¯¼å‡ºç¨³å®šæ€§)
        self.gru = nn.GRU(
            embed_dim, 
            hidden_dim, 
            bidirectional=True, 
            batch_first=True, 
            num_layers=2, 
            dropout=0.5 if self.training else 0 # å¯¼å‡ºæ¨¡å‹å‰åŠ¡å¿… model.eval()
        )
        
        # 4. Attention (æ³¨æ„åŠ›æœºåˆ¶)
        self.attention_linear = nn.Linear(hidden_dim * 2, 1)
        
        # 5. Output
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 4, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x: [B, L]
        emb = self.embedding(x) # [B, L, E]
        
        # CNN å¤„ç† - æ˜¾å¼å¤„ç† Permute é€»è¾‘
        cnn_in = emb.transpose(1, 2) # [B, E, L]
        cnn_out = self.conv1(cnn_in)
        cnn_out = self.relu(cnn_out).transpose(1, 2) # [B, L, E]
        
        # æ®‹å·®è¿æ¥ä¸è§„èŒƒåŒ–
        rnn_in = self.norm1(emb + cnn_out)
        
        # GRU å¤„ç† (æ˜¾å¼æ¥æ”¶å¹¶å¿½ç•¥ hidden state æ›´æœ‰åˆ©äº ONNX é™æ€å›¾è¿½è¸ª)
        self.gru.flatten_parameters() 
        gru_out, _ = self.gru(rnn_in) # [B, L, H*2]
        
        # Attention è®¡ç®—
        # ä½¿ç”¨ tanh å¢åŠ éçº¿æ€§ï¼Œç¡®ä¿å¾—åˆ†åˆ†å¸ƒåœ¨ -1 åˆ° 1 ä¹‹é—´ï¼Œå¢å¼ºæ•°å€¼ç¨³å®šæ€§
        attn_scores = torch.tanh(self.attention_linear(gru_out)) # [B, L, 1]
        attn_weights = F.softmax(attn_scores, dim=1) # [B, L, 1]
        
        # ä¸Šä¸‹æ–‡å‘é‡ (Context Vector)
        context = torch.sum(gru_out * attn_weights, dim=1) # [B, H*2]
        
        # ä¼˜åŒ–ï¼šä½¿ç”¨ expand æ›¿ä»£ repeat ä»¥é™ä½ ONNX è½¬æ¢è¯¯å·®
        # expand ä¸ä¼šå¤åˆ¶å†…å­˜ï¼Œåœ¨ ONNX ä¸­ä¼šè½¬æ¢ä¸ºé«˜æ•ˆçš„ Broadcast ç®—å­
        batch_size, seq_len, _ = gru_out.shape
        context_expanded = context.unsqueeze(1).expand(-1, seq_len, -1) # [B, L, H*2]
        
        # æ‹¼æ¥ï¼š[B, L, H*4]
        combined = torch.cat([gru_out, context_expanded], dim=2) 
        
        # è¾“å‡ºï¼š[B, L]
        # æ³¨æ„ï¼šsqueeze(-1) åœ¨ batch ä¸º 1 æ—¶å¯èƒ½å¯¼è‡´ç»´åº¦å¡Œé™·ï¼Œå»ºè®®æ˜¾å¼æŒ‡å®šç»´åº¦
        out = self.fc(combined).view(batch_size, seq_len)
        return out
        
# --- æ ‡å‡†åŠ æƒäº¤å‰ç†µ Loss (æ¯” Focal Loss æ›´ç¨³å¥) ---
class WeightedBCELoss(nn.Module):
    def __init__(self, pos_weight=4.0, reduction='mean'):
        """
        Args:
            pos_weight (float): æ­£æ ·æœ¬(ç”µå½±å)çš„æƒé‡å€æ•°ã€‚
                                é»˜è®¤ä¸º 4.0ï¼Œæ„å‘³ç€ä¸€ä¸ªç”µå½±åå­—ç¬¦çš„é‡è¦æ€§æ˜¯ä¸€ä¸ªèƒŒæ™¯å­—ç¬¦çš„ 4 å€ã€‚
            reduction (str): 'mean' æˆ– 'sum'
        """
        super(WeightedBCELoss, self).__init__()
        self.pos_weight = pos_weight
        self.reduction = reduction

    def forward(self, inputs, targets):
        # 1. è®¡ç®—æ ‡å‡†çš„äºŒåˆ†ç±»äº¤å‰ç†µ (ä¸æ±‚å¹³å‡ï¼Œä¿ç•™æ¯ä¸ªç‚¹çš„ loss)
        # inputs: æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡ (0~1)
        # targets: çœŸå®æ ‡ç­¾ (0 æˆ– 1)
        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')

        # 2. ç”Ÿæˆæƒé‡å‘é‡
        # å¦‚æœ target æ˜¯ 1 (æ­£æ ·æœ¬)ï¼Œæƒé‡ = self.pos_weight
        # å¦‚æœ target æ˜¯ 0 (è´Ÿæ ·æœ¬)ï¼Œæƒé‡ = 1.0
        weights = targets * self.pos_weight + (1 - targets)

        # 3. åŠ æƒ
        weighted_loss = bce_loss * weights

        # 4. å½’çº¦è¿”å›
        if self.reduction == 'mean':
            return weighted_loss.mean()
        elif self.reduction == 'sum':
            return weighted_loss.sum()
        else:
            return weighted_loss

# --- Focal Loss ---
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.75, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha   # å¢å¤§æ­£æ ·æœ¬(ç”µå½±åå­—ç¬¦)çš„æƒé‡
        self.gamma = gamma   # èšç„¦éš¾åˆ†ç±»æ ·æœ¬
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.binary_cross_entropy(inputs, targets, reduction="none")
        p_t = inputs * targets + (1 - inputs) * (1 - targets)
        loss = ce_loss * ((1 - p_t) ** self.gamma)

        if self.alpha >= 0:
            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
            loss = alpha_t * loss

        if self.reduction == "mean": return loss.mean()
        elif self.reduction == "sum": return loss.sum()
        return loss

# --- æ•°æ®é›†å®šä¹‰ ---
class MovieDataset(Dataset):
    PREFIX_LIST = [
        'Download', 'Downloads', 'Movies', 'TV_Shows', 'Media', 'Video', 
        'Temp', 'Backup', 'Data', 'New_Folder', 'Private',
        'ä¸‹è½½', 'è¿…é›·ä¸‹è½½', 'ç™¾åº¦ç½‘ç›˜', 'æˆ‘çš„ä¸‹è½½', 'ä¼ è¾“æ–‡ä»¶', 'æ¥æ”¶æ–‡ä»¶',
        'ç”µå½±', 'ç”µè§†å‰§', 'è§†é¢‘', 'å½±è§†', 'å‰§é›†', 'åŠ¨æ¼«', 'åŠ¨ç”»', 
        'ç»¼è‰º', 'çºªå½•ç‰‡', 'åˆé›†', 'å›½äº§å‰§', 'ç¾å‰§', 'æ—¥å‰§', 'éŸ©å‰§',
        'æ–°å»ºæ–‡ä»¶å¤¹', 'å¤‡ä»½', 'ä¸´æ—¶', 'èµ„æ–™', 'æ¡Œé¢', 'æˆ‘çš„æ–‡æ¡£', 
        'æ”¶è—', 'å¾…çœ‹', 'å·²çœ‹', 'æ•´ç†', 'å›æ”¶ç«™'
    ]
    
    SUFFIX_LIST = [
        'mp4', 'mkv', 'avi', 'rmvb', 'wmv', 'mov', 'flv', 'iso', 'torrent',
        '1080p', '720p', '2160p', '4K', 'x264', 'x265', 'HEVC', 
        'HDR', 'BluRay', 'BDrip', 'WebDL', 'HDTV', 'AAC', 'DTS', 'Atmos',
        'é«˜æ¸…', 'è“å…‰', 'å­—å¹•', 'ç¦åˆ©', 'å®Œç»“', 'æœªåˆ å‡', 'åŠ é•¿ç‰ˆ',
        'ä¸­å­—', 'åŒè¯­', 'å›½è¯­', 'ç‰¹æ•ˆå­—å¹•', 'å«èŠ±çµ®', 'ä¿®æ­£ç‰ˆ',
        'Backup', 'Copy', 'Temp', 'å‰¯æœ¬'
    ]
    def __init__(self, lines, char_to_idx, max_len=MAX_LEN, training=True):
        self.samples = []
        self.char_to_idx = char_to_idx
        self.max_len = max_len
        self.training = training  # æ§åˆ¶æ˜¯å¦å¼€å¯éšæœºå¢å¼º
        
        skipped_count = 0
        
        # 1. åœ¨ Init ä¸­ä»…åšæœ‰æ•ˆæ€§ç­›é€‰ï¼Œä¿å­˜åŸå§‹æ–‡æœ¬
        for line in lines:
            line = line.strip()
            if '#' not in line: continue
            input_path, target_name = line.rsplit('#', 1)
            target_name = target_name.strip()
            
            # é¢„æ£€æŸ¥ï¼šç¡®ä¿åŸå§‹æ•°æ®æ˜¯èƒ½åŒ¹é…ä¸Šçš„
            escaped_target = re.escape(target_name)
            pattern = escaped_target.replace(r'\ ', r'[._\s]+')
            if re.search(pattern, input_path, re.IGNORECASE):
                self.samples.append((input_path, target_name))
            else:
                skipped_count += 1

        if skipped_count > 0:
            print(f"Dataset Info: è·³è¿‡äº† {skipped_count} æ¡æ— æ³•åŒ¹é…æ ‡ç­¾çš„æ•°æ®ã€‚")

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx):
        # 2. è·å–åŸå§‹æ•°æ®
        input_path, target_name = self.samples[idx]
        
        # 3. éšæœºè·¯å¾„å¢å¼º (ä»…åœ¨è®­ç»ƒæ¨¡å¼ä¸‹)
        if random.random() < 0.3:
            # 50% æ¦‚ç‡åŠ å‰ç¼€ï¼ˆæ¨¡æ‹Ÿç›®å½•ï¼‰ï¼Œ50% æ¦‚ç‡åŠ åç¼€ï¼ˆæ¨¡æ‹Ÿæ–‡ä»¶å±æ€§ï¼‰
            if random.random() < 0.5:
                # --- æƒ…å†µ 1ï¼šåŠ å‰ç¼€ (æ¨¡æ‹Ÿçˆ¶çº§ç›®å½•) ---
                noise = random.choice(self.PREFIX_LIST)
                input_path = f"{noise}/{input_path}"
            else:
                # --- æƒ…å†µ 2ï¼šåŠ åç¼€ (æ¨¡æ‹Ÿæ–‡ä»¶å±æ€§/æ ‡ç­¾) ---
                noise = random.choice(self.SUFFIX_LIST)
                # åç¼€åˆ†éš”ç¬¦ä¸»è¦æ˜¯ç‚¹ã€ä¸‹åˆ’çº¿ã€ç©ºæ ¼ã€çŸ­æ¨ªçº¿
                sep = random.choice(['.', '_', ' ', '-'])
                input_path = f"{input_path}{sep}{noise}"

            # === Part B: åˆ†éš”ç¬¦æ‰°åŠ¨ ===
            if random.random() < 0.3:
                input_path = input_path.replace('.', ' ')
            elif random.random() < 0.3:
                input_path = input_path.replace('_', ' ')
            elif random.random() < 0.2:
                input_path = input_path.replace(' ', '.')

        # 4. å®æ—¶è®¡ç®—ç´¢å¼• (åŒ¹é…æœ€åä¸€æ¬¡å‡ºç°çš„ç›®æ ‡åï¼Œä¼˜å…ˆæ–‡ä»¶å)
        escaped_target = re.escape(target_name)
        pattern = escaped_target.replace(r'\ ', r'[._\s]+')

        # æ‰¾æ‰€æœ‰åŒ¹é…é¡¹ï¼Œå–æœ€åä¸€ä¸ª
        matches = list(re.finditer(pattern, input_path, re.IGNORECASE))
        match = matches[-1] if matches else None

        # å…œåº•ï¼šå¦‚æœåŒ¹é…å¤±è´¥ï¼ˆæå°‘è§ï¼‰ï¼Œå›é€€åˆ°åŸå§‹è·¯å¾„é‡æ–°åŒ¹é…
        if not match:
            input_path, _ = self.samples[idx]
            matches = list(re.finditer(pattern, input_path, re.IGNORECASE))
            match = matches[-1] if matches else None

        start_idx = match.start()
        end_idx = match.end()
        
        # 5. è½¬ Tensor å’Œ Padding
        # æˆªæ–­è¾“å…¥ï¼Œé˜²æ­¢å¢å¼ºåé•¿åº¦æº¢å‡º
        input_ids = [self.char_to_idx.get(c.lower(), 1) for c in input_path[:self.max_len]]
        labels = [0.0] * len(input_ids)
        
        limit = min(end_idx, self.max_len)
        for i in range(start_idx, limit):
            labels[i] = 1.0
        
        pad_len = self.max_len - len(input_ids)
        
        # ç¡®ä¿ pad_len ä¸ä¸ºè´Ÿæ•°
        pad_len = max(0, pad_len)
        
        return (
            torch.tensor(input_ids + [0] * pad_len), 
            torch.tensor(labels + [0.0] * pad_len)
        )

# --- è¾…åŠ©å‡½æ•°ï¼šéªŒè¯é›†è®¡ç®— ---
def validate_one_epoch(model, loader, criterion):
    model.eval()
    v_loss = 0
    with torch.no_grad():
        for vx, vy in loader:
            pred = model(vx)
            loss = criterion(pred, vy)
            v_loss += loss.item()
    return v_loss / len(loader) if len(loader) > 0 else 0

# --- è®­ç»ƒé€»è¾‘ ---
def run_train(incremental=False):
    # set_seed(SEED)
    mode_str = "ã€å¢é‡è®­ç»ƒæ¨¡å¼ã€‘" if incremental else "ã€å…¨é‡è®­ç»ƒæ¨¡å¼ã€‘"
    print(f"{mode_str}")

    # 1. æœç´¢æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
    data_files = glob.glob(DATA_FILE_PATTERN)
    # æ’åºä»¥ä¿è¯æ¯æ¬¡è¿è¡Œè¯»å–é¡ºåºä¸€è‡´ï¼Œç¡®ä¿ index 0 æ€»æ˜¯åŒä¸€ä¸ªæ–‡ä»¶
    data_files.sort()
    
    if not data_files:
        print(f"âŒ æœªæ‰¾åˆ°åŒ¹é… {DATA_FILE_PATTERN} çš„æ•°æ®æ–‡ä»¶ã€‚"); return
    
    print(f"å‘ç° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶: {data_files}")

    all_train_lines = []
    all_val_lines = []
    
    # ä½¿ç”¨ç‹¬ç«‹çš„ Random å®ä¾‹è¿›è¡Œ shuffleï¼Œä¸å½±å“å…¨å±€çŠ¶æ€
    rng = random.Random(SEED)
    
    # 2. éå†æ¯ä¸ªæ–‡ä»¶
    for i, f_path in enumerate(data_files):
        with open(f_path, 'r', encoding='utf-8') as f:
            lines = [l.strip() for l in f.readlines() if '#' in l.strip()]
        
        # æ­¥éª¤A: ç¡®å®šæ€§æ‰“ä¹±
        rng.shuffle(lines)
        total_raw = len(lines)
        if total_raw == 0: continue
        
        # æ­¥éª¤B: å…ˆè¿›è¡Œ è®­ç»ƒ/éªŒè¯ åˆ‡åˆ†
        # æ— è®ºæ˜¯å¦å¢é‡ï¼Œæ°¸è¿œå›ºå®šå‰90%ä¸ºè®­ç»ƒæ± ï¼Œå10%ä¸ºéªŒè¯æ± ã€‚
        # è¿™æ ·å¯ä»¥ä¿è¯éªŒè¯é›†æ°¸è¿œçº¯å‡€ï¼Œä¸ä¼šå› ä¸ºå¢é‡è£åˆ‡å¯¼è‡´è®­ç»ƒæ•°æ®è¶Šç•Œã€‚
        split_idx = int(total_raw * 0.9)
        if split_idx == 0 and total_raw > 0: split_idx = total_raw # æå°‘æ•°æ®ä¿æŠ¤
        
        file_train_lines = lines[:split_idx]
        file_val_lines = lines[split_idx:]
        
        # æ­¥éª¤C: å¤„ç†å¢é‡é€»è¾‘ï¼ˆä»…åœ¨åˆ‡åˆ†åçš„å„è‡ªæ± å­å†…è¿›è¡Œä¿ç•™/ä¸¢å¼ƒï¼‰
        if incremental and i == 0:
            # æ—§æ–‡ä»¶ï¼šä»…ä¿ç•™ 2% çš„è®­ç»ƒæ•°æ®ï¼Œä»¥åŠ 2% çš„éªŒè¯æ•°æ® (ä¿æŒåˆ†å¸ƒä¸€è‡´ï¼Œä¸”èŠ‚çœéªŒè¯æ—¶é—´)
            keep_train_count = int(len(file_train_lines) * 0.02)
            keep_val_count = int(len(file_val_lines) * 0.02)
            
            # æœ€å°ä¿ç•™ä¿æŠ¤
            if keep_train_count == 0 and len(file_train_lines) > 0: keep_train_count = 1
            if keep_val_count == 0 and len(file_val_lines) > 0: keep_val_count = 1
            
            final_train = file_train_lines[:keep_train_count]
            final_val = file_val_lines[:keep_val_count]
            
            print(f"  â””â”€ [Old Data] {os.path.basename(f_path)}: é‡‡æ ·ä¿ç•™ è®­ç»ƒ{len(final_train)}æ¡ / éªŒè¯{len(final_val)}æ¡")
        else:
            # æ–°æ–‡ä»¶æˆ–å…¨é‡æ¨¡å¼ï¼šä¿ç•™åˆ‡åˆ†åçš„æ‰€æœ‰æ•°æ®
            final_train = file_train_lines
            final_val = file_val_lines
            print(f"  â””â”€ [New Data] {os.path.basename(f_path)}: å…¨é‡è¯»å– è®­ç»ƒ{len(final_train)}æ¡ / éªŒè¯{len(final_val)}æ¡")

        all_train_lines.extend(final_train)
        all_val_lines.extend(final_val)

    print(f"\næ•°æ®é›†å‡†å¤‡å®Œæ¯•: è®­ç»ƒé›† {len(all_train_lines)} æ¡ | éªŒè¯é›† {len(all_val_lines)} æ¡")

    # 4. æ„å»ºæˆ–åŠ è½½è¯è¡¨
    all_lines_for_vocab = all_train_lines + all_val_lines
    if os.path.exists(VOCAB_PATH):
        with open(VOCAB_PATH, 'rb') as f: char_to_idx = pickle.load(f)
        print("å·²åŠ è½½ç°æœ‰è¯è¡¨ã€‚")
    else:
        # å¼ºè¡Œæ³¨å…¥åŸºç¡€ ASCII å­—ç¬¦ï¼Œé˜²æ­¢è‹±æ–‡è¯è¡¨ç¼ºå¤±
        raw_paths = [l.split('#')[0] for l in all_train_lines + all_val_lines]
        all_chars = set("".join(raw_paths).lower())
        
        ascii_chars = set(string.ascii_lowercase + string.digits + string.punctuation + " ")
        all_chars.update(ascii_chars)
        
        char_to_idx = {c: i+2 for i, c in enumerate(sorted(list(all_chars)))}
        char_to_idx['<PAD>'], char_to_idx['<UNK>'] = 0, 1
        with open(VOCAB_PATH, 'wb') as f: pickle.dump(char_to_idx, f)
        print(f"å·²åˆ›å»ºæ–°è¯è¡¨ï¼ŒåŒ…å« {len(char_to_idx)} ä¸ªå­—ç¬¦ã€‚")

    # 5. åˆ›å»º Dataset å’Œ DataLoader
    train_ds = MovieDataset(all_train_lines, char_to_idx, training=True)
    val_ds = MovieDataset(all_val_lines, char_to_idx, training=False)


    if len(train_ds) < 1:
        print("æœ‰æ•ˆæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒã€‚"); return
    # g = torch.Generator()
    # g.manual_seed(SEED)
    #train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, generator=g, num_workers=min(4, NUM_THREADS))
    
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=min(4, NUM_THREADS))
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=min(4, NUM_THREADS))

    # åˆå§‹åŒ–æ–°æ¨¡å‹
    model = Extractor(len(char_to_idx), embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM)
    
    # ä½¿ç”¨ Focal Loss
    #criterion = FocalLoss(alpha=0.75, gamma=2)
    criterion = WeightedBCELoss(pos_weight=4.0, reduction='mean')
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5) # å¢åŠ  weight_decay é˜²æ­¢è¿‡æ‹Ÿåˆ
    
    best_val_loss = float('inf')

    # åŠ è½½æ¨¡å‹é€»è¾‘
    if os.path.exists(MODEL_PATH):
        print(f"æ£€æµ‹åˆ°ç°æœ‰æ¨¡å‹ï¼ŒåŠ è½½æƒé‡ä»¥ LR={LR} ç»§ç»­å¾®è°ƒ...")
        model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
        
        if len(val_ds) > 0:
            print("æ­£åœ¨è®¡ç®—å½“å‰æ¨¡å‹çš„åˆå§‹éªŒè¯é›† Loss (åŸºå‡†çº¿)...")
            initial_val_loss = validate_one_epoch(model, val_loader, criterion)
            best_val_loss = initial_val_loss 
            print(f"å½“å‰æ¨¡å‹åŸºå‡† Loss: {best_val_loss:.4f}")
    else:
        print("æœªæ£€æµ‹åˆ°æ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
    
    try:
        for epoch in range(EPOCHS):
            model.train()
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1:02d}")
            for x, y in pbar:
                optimizer.zero_grad()
                pred = model(x)
                loss = criterion(pred, y)
                loss.backward()
                # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
                optimizer.step()
                pbar.set_postfix(loss=f"{loss.item():.4f}")
            
            if len(val_ds) > 0:
                avg_val_loss = validate_one_epoch(model, val_loader, criterion)
                
                if avg_val_loss < best_val_loss:
                    print(f" âœ¨ éªŒè¯é›† Loss ä¼˜åŒ– ({best_val_loss:.4f} -> {avg_val_loss:.4f})ï¼Œæ¨¡å‹å·²æ›´æ–°ã€‚")
                    best_val_loss = avg_val_loss
                    torch.save(model.state_dict(), MODEL_PATH)
                else:
                    print(f" â³ éªŒè¯é›† Loss: {avg_val_loss:.4f} (æœªæå‡ï¼Œæœ€ä½³: {best_val_loss:.4f})")
            else:
                torch.save(model.state_dict(), MODEL_PATH)
                print(" âš ï¸ æ— éªŒè¯é›†ï¼Œæ¨¡å‹å·²ä¿å­˜ã€‚")
                
    except KeyboardInterrupt: print("\nğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨åœæ­¢è®­ç»ƒã€‚")

# --- ä¸€æ¬¡æ€§åˆå§‹åŒ–æ¨¡å‹ ---
def init_model_and_vocab():
    """åˆå§‹åŒ–æ¨¡å‹å’Œè¯è¡¨ï¼Œè¿”å› (model, char_to_idx)ï¼Œæ‰¹é‡é¢„æµ‹æ—¶ä»…è°ƒç”¨1æ¬¡"""
    if not os.path.exists(MODEL_PATH) or not os.path.exists(VOCAB_PATH):
        print("é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æˆ–è¯è¡¨æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œè®­ç»ƒã€‚")
        return None, None
    
    # åŠ è½½è¯è¡¨ï¼ˆä»…1æ¬¡ï¼‰
    with open(VOCAB_PATH, 'rb') as f:
        char_to_idx = pickle.load(f)
    
    # åˆå§‹åŒ–æ¨¡å‹ + åŠ è½½æƒé‡ï¼ˆä»…1æ¬¡ï¼‰
    model = Extractor(len(char_to_idx), embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM)
    model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
    model.eval()  # é¢„æµ‹æ¨¡å¼ï¼Œç¦ç”¨Dropout/BatchNorm
    
    return model, char_to_idx

def predict_single_path(path, model, char_to_idx):
    """å•æ¡è·¯å¾„é¢„æµ‹ï¼Œå¤ç”¨å·²åˆå§‹åŒ–çš„æ¨¡å‹"""
    if '#' in path:
        print(path)
        return
    
    # è¾“å…¥é¢„å¤„ç†ï¼ˆä»…å¤„ç†å½“å‰è·¯å¾„ï¼Œæ— é‡å¤åŠ è½½ï¼‰
    input_ids = [char_to_idx.get(c.lower(), 1) for c in path[:MAX_LEN]]
    padded = input_ids + [0] * (MAX_LEN - len(input_ids))
    
    with torch.no_grad():
        probs = model(torch.tensor([padded]))[0][:len(path)].numpy()

    # --- åå¤„ç†ç­–ç•¥ ---
    selected_mask = [False] * len(probs)
    
    # 1. é˜ˆå€¼ç­›é€‰
    for i, p in enumerate(probs):
        if p > THRESHOLD: selected_mask[i] = True
            
    # 2. ç©ºæ´å¡«è¡¥ (Gap Filling) - ä¿®å¤ "Iron.Man" ä¸­é—´æ–­å¼€çš„é—®é¢˜
    gap_limit = 2 
    for i in range(len(probs)):
        if selected_mask[i]:
            # å¯»æ‰¾ä¸‹ä¸€ä¸ªè¢«é€‰ä¸­çš„ç‚¹
            for j in range(i + 1, min(i + gap_limit + 2, len(probs))):
                if selected_mask[j]:
                    # å°†ä¸­é—´æ‰€æœ‰éè·¯å¾„åˆ†éš”ç¬¦çš„å­—ç¬¦éƒ½è¿èµ·æ¥
                    for k in range(i + 1, j):
                        if path[k] not in ['/', '\\']:
                            selected_mask[k] = True
                    break

    res_list = []

    if DEBUG_MODE:
        print(f"\n{'='*65}")
        print(f"{'ç´¢å¼•':<4} | {'å­—ç¬¦':<4} | {'åˆ†å€¼':<15} | çŠ¶æ€")
        print("-" * 65)
        for i, p in enumerate(probs):
            status = "âœ… [é€‰ä¸­]" if p > THRESHOLD else "   [æ’é™¤]"
            print(f"{i:<4} | {path[i]:<4} | {p:.10f} | {status}")
        print(f"{'='*65}\n")

    
    for i, is_sel in enumerate(selected_mask):
        if is_sel:
            res_list.append(path[i])

    raw_result = "".join(res_list)
    clean_result = raw_result.replace('.', ' ').replace('_', ' ')
    clean_result = re.sub(r'\s+', ' ', clean_result)
    clean_result = clean_result.strip("/()# â€œâ€.-")

    # 1. éªŒè¯è¿ç»­æ€§
    if clean_result:
        escaped_clean = re.escape(clean_result)
        verify_pattern = escaped_clean.replace(r'\ ', r'[._\s\-\(\)\[\]]*')
        if not re.search(verify_pattern, path, re.IGNORECASE):
            if DEBUG_MODE:
                print(f"[éªŒè¯å¤±è´¥] '{clean_result}' æ— æ³•åœ¨åŸè·¯å¾„ä¸­è¿ç»­åŒ¹é…ï¼Œåˆ¤å®šä¸ºæ— æ•ˆæå–ã€‚")
            clean_result = ""

    # 2. æ··åˆæ¨¡å¼ä¿®å¤
    if clean_result:
        clean_result = TextUtils.fix_name(path, clean_result) 

    if DEBUG_MODE: 
        print(f"æå–åŸæ–‡: {raw_result}")
        print(f"æœ€ç»ˆç»“æœ: {clean_result}")
    else: 
        if clean_result:
            print(f"{path}#{clean_result}")
        else:
            print(f"{path}#")

def run_batch_predict(file_path):
    # 1. ä¸€æ¬¡æ€§åˆå§‹åŒ–æ¨¡å‹å’Œè¯è¡¨
    model, char_to_idx = init_model_and_vocab()
    if model is None or char_to_idx is None:
        return
    
    # 2. è¯»å–æ–‡ä»¶ä¸­çš„æ‰€æœ‰è·¯å¾„
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # 3. å¾ªç¯é¢„æµ‹æ‰€æœ‰è·¯å¾„
    total_lines = len(lines)
    for idx, line in enumerate(lines):
        predict_single_path(line, model, char_to_idx)

# --- å…¥å£æ§åˆ¶ ---
if __name__ == "__main__":
    if os.path.exists("dbg"):
        DEBUG_MODE = True
        print(f"æ£€æµ‹åˆ° [dbg] æ–‡ä»¶ï¼Œå·²å¼ºåˆ¶å¼€å¯è°ƒè¯•æ¨¡å¼")

    if len(sys.argv) > 1:
        input_arg = sys.argv[1]

        if input_arg == '--inc':
            # æ¨¡å¼ 1: å¢é‡è®­ç»ƒ
            run_train(incremental=True)
        
        elif os.path.exists(input_arg) and os.path.isfile(input_arg):
            # æ¨¡å¼ 2: æ‰¹é‡é¢„æµ‹
            run_batch_predict(input_arg)
        
        else:
            # æ¨¡å¼ 3: å•æ¡å­—ç¬¦ä¸²é¢„æµ‹
            model, char_to_idx = init_model_and_vocab()
            if model and char_to_idx:
                predict_single_path(input_arg, model, char_to_idx)
    else:
        # æ¨¡å¼ 4: é»˜è®¤å…¨é‡è®­ç»ƒ
        run_train(incremental=False)
