import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import pickle
import sys
import os
import re
import random
import numpy as np
import glob
import string

# --- å…¨å±€æ ¸å¿ƒé…ç½® ---
NUM_THREADS = 4
BATCH_SIZE = 128
LR = 1e-3            # å­¦ä¹ ç‡
EPOCHS = 50          # è®­ç»ƒè½®æ•°
MAX_LEN = 300        # æœ€å¤§åºåˆ—é•¿åº¦
EMBED_DIM = 64      # å‘é‡ç»´åº¦
HIDDEN_DIM = 128     # éšè—å±‚ç»´åº¦

MODEL_PATH = "movie_model.pth"
VOCAB_PATH = "vocab.pkl"
# æ•°æ®æ–‡ä»¶åŒ¹é…æ¨¡å¼ (åŒ¹é… train_data.txt, train_data_2.txt ç­‰)
DATA_FILE_PATTERN = "train_data*.txt" 
SEED = 42            # ğŸ² å›ºå®šéšæœºç§å­

# --- ğŸ” é¢„æµ‹/è°ƒè¯•é…ç½® ---
DEBUG_MODE = False    # å¼€å¯è°ƒè¯•è¯¦æƒ…
THRESHOLD = 0.35     # æé«˜åˆ¤å®šé˜ˆå€¼ï¼Œå‡å°‘å™ªéŸ³
SMOOTH_VAL = 0.1     # å¹³æ»‘æ•‘å›é˜ˆå€¼

# è®¾ç½®çº¿ç¨‹æ•°
torch.set_num_threads(NUM_THREADS)

# --- ğŸ› ï¸ è¾…åŠ©å·¥å…·ç±» ---
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# --- ç§»æ¤çš„ JS é€»è¾‘å·¥å…·ç±» ---
class TextUtils:
    CN_NUMS = ["é›¶", "ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å"]

    @staticmethod
    def number2text(text):
        if not text: return text
        text = text.lstrip('0')
        if not text: return "é›¶" 

        try:
            num = int(text)
        except ValueError:
            return text

        if num <= 10:
            return TextUtils.CN_NUMS[num]
        elif num < 20:
            return "å" + TextUtils.CN_NUMS[num % 10]
        elif num % 10 == 0:
            return TextUtils.CN_NUMS[num // 10] + "å"
        else:
            return TextUtils.CN_NUMS[num // 10] + "å" + TextUtils.CN_NUMS[num % 10]

    @staticmethod
    def fix_name(path, ai_result):
        replace_patterns = [
            r'Season\s*(\d{1,2})',              
            r'SE(\d{1,2})',                     
            r'(?<![a-zA-Z])S(\d{1,2})(?![a-zA-Z])', 
            r'ç¬¬(\d{1,2})å­£'                    
        ]

        processed_result = ai_result
        replaced_flag = False 

        def replace_func(match):
            nonlocal replaced_flag
            replaced_flag = True
            num = match.group(1)
            cn_num = TextUtils.number2text(num)
            return f" ç¬¬{cn_num}å­£ " 

        for pattern in replace_patterns:
            if re.search(pattern, processed_result, re.IGNORECASE):
                processed_result = re.sub(pattern, replace_func, processed_result, flags=re.IGNORECASE)
        
        processed_result = re.sub(r'\s+', ' ', processed_result).strip()

        if replaced_flag:
            return processed_result

        path_search_patterns = [
            r'Season\s*(\d{1,2})',
            r'SE(\d{1,2})',
            r'ç¬¬(\d{1,2})å­£',
            r'(?<![A-Za-z])S(\d{1,2})'
        ]

        for pattern in path_search_patterns:
            match = re.search(pattern, path, re.IGNORECASE)
            if match:
                num = match.group(1)
                cn_num = TextUtils.number2text(num)
                suffix = f"ç¬¬{cn_num}å­£"
                if suffix not in processed_result:
                    return f"{processed_result} {suffix}"
                break 
        
        return processed_result

# --- æ¨¡å‹ç»“æ„ (CNN + BiGRU + Attention) ---
class Extractor(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):
        super().__init__()
        
        # 1. Embedding
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # 2. CNN (æå–å±€éƒ¨ n-gram ç‰¹å¾ï¼Œå¦‚ "The", "Man")
        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.norm1 = nn.LayerNorm(embed_dim) # ç”¨äºæ®‹å·®è¿æ¥
        
        # 3. BiGRU (æå–åºåˆ—é•¿è·ç¦»ä¾èµ–)
        self.gru = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True, num_layers=2, dropout=0.5)
        
        # 4. Attention (æ³¨æ„åŠ›æœºåˆ¶)
        self.attention_linear = nn.Linear(hidden_dim * 2, 1)
        
        # 5. Output
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 4, 128), # hidden*2(GRU) + hidden*2(Context)
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x: [B, L]
        emb = self.embedding(x) # [B, L, E]
        
        # CNN å¤„ç†
        cnn_in = emb.permute(0, 2, 1) # [B, E, L]
        cnn_out = self.conv1(cnn_in)
        cnn_out = self.relu(cnn_out).permute(0, 2, 1) # [B, L, E]
        
        # æ®‹å·®è¿æ¥ï¼šä¿ç•™åŸå§‹å­—ç¬¦ç‰¹å¾ + CNNæå–çš„å±€éƒ¨ç‰¹å¾
        rnn_in = self.norm1(emb + cnn_out)
        
        # GRU å¤„ç†
        gru_out, _ = self.gru(rnn_in) # [B, L, H*2]
        
        # Attention è®¡ç®—
        attn_scores = torch.tanh(self.attention_linear(gru_out)) # [B, L, 1]
        attn_weights = F.softmax(attn_scores, dim=1)
        
        # ä¸Šä¸‹æ–‡å‘é‡ (Context Vector)
        context = torch.sum(gru_out * attn_weights, dim=1) # [B, H*2]
        
        # æ‹¼æ¥ï¼šæ¯ä¸ªæ—¶é—´æ­¥éƒ½ç»“åˆå…¨å±€ä¸Šä¸‹æ–‡
        seq_len = gru_out.size(1)
        context_expanded = context.unsqueeze(1).repeat(1, seq_len, 1) # [B, L, H*2]
        
        combined = torch.cat([gru_out, context_expanded], dim=2) # [B, L, H*4]
        
        return self.fc(combined).squeeze(-1)

# --- Focal Loss ---
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.75, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha   # å¢å¤§æ­£æ ·æœ¬(ç”µå½±åå­—ç¬¦)çš„æƒé‡
        self.gamma = gamma   # èšç„¦éš¾åˆ†ç±»æ ·æœ¬
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.binary_cross_entropy(inputs, targets, reduction="none")
        p_t = inputs * targets + (1 - inputs) * (1 - targets)
        loss = ce_loss * ((1 - p_t) ** self.gamma)

        if self.alpha >= 0:
            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
            loss = alpha_t * loss

        if self.reduction == "mean": return loss.mean()
        elif self.reduction == "sum": return loss.sum()
        return loss

# --- æ•°æ®é›†å®šä¹‰ ---
class MovieDataset(Dataset):
    def __init__(self, lines, char_to_idx, max_len=MAX_LEN, training=True):
        self.samples = []
        self.char_to_idx = char_to_idx
        self.max_len = max_len
        self.training = training  # æ§åˆ¶æ˜¯å¦å¼€å¯éšæœºå¢å¼º
        
        skipped_count = 0
        
        # 1. åœ¨ Init ä¸­ä»…åšæœ‰æ•ˆæ€§ç­›é€‰ï¼Œä¿å­˜åŸå§‹æ–‡æœ¬
        for line in lines:
            line = line.strip()
            if '#' not in line: continue
            input_path, target_name = line.rsplit('#', 1)
            target_name = target_name.strip()
            
            # é¢„æ£€æŸ¥ï¼šç¡®ä¿åŸå§‹æ•°æ®æ˜¯èƒ½åŒ¹é…ä¸Šçš„
            escaped_target = re.escape(target_name)
            pattern = escaped_target.replace(r'\ ', r'[._\s]+')
            if re.search(pattern, input_path, re.IGNORECASE):
                self.samples.append((input_path, target_name))
            else:
                skipped_count += 1

        if skipped_count > 0:
            print(f"Dataset Info: è·³è¿‡äº† {skipped_count} æ¡æ— æ³•åŒ¹é…æ ‡ç­¾çš„æ•°æ®ã€‚")

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx):
        # 2. è·å–åŸå§‹æ•°æ®
        input_path, target_name = self.samples[idx]
        
        # 3. ğŸ² éšæœºè·¯å¾„å¢å¼º (ä»…åœ¨è®­ç»ƒæ¨¡å¼ä¸‹)
        if self.training:
            # === Part A: å™ªå£°æ³¨å…¥ (è·¯å¾„å¼€å¤´æˆ–ç»“å°¾åŠ æ— å…³è¯) ===
            if random.random() < 0.3:
                noise_list = ['Download', 'Movies', 'Temp', 'Backup', 'Data', '1080p', 'x264', 'New_Folder']
                noise = random.choice(noise_list)
                
                if random.random() < 0.5:
                    # åŠ åœ¨å¼€å¤´ï¼šæ¨¡æ‹Ÿå¤šäº†ä¸€å±‚ç›®å½• (e.g., "Download/åŸå§‹è·¯å¾„")
                    sep = random.choice(['/', '\\', '.'])
                    input_path = f"{noise}{sep}{input_path}"
                else:
                    # åŠ åœ¨ç»“å°¾ï¼šæ¨¡æ‹Ÿå¤šäº†ä¸€äº›åç¼€ä¿¡æ¯ (e.g., "åŸå§‹è·¯å¾„.1080p")
                    sep = random.choice(['.', '_', ' '])
                    input_path = f"{input_path}{sep}{noise}"

            # === Part B: åˆ†éš”ç¬¦æ‰°åŠ¨ ===
            if random.random() < 0.3:
                input_path = input_path.replace('.', ' ')
            elif random.random() < 0.3:
                input_path = input_path.replace('_', ' ')
            elif random.random() < 0.2:
                input_path = input_path.replace(' ', '.')
            
            # âŒ é”™è¯¯ä»£ç å·²åˆ é™¤ï¼š return input_path 
            # âœ… æ­£ç¡®é€»è¾‘ï¼šä¿®æ”¹å®Œ input_path åï¼Œä¸è¿”å›ï¼Œç»§ç»­å¾€ä¸‹èµ°ï¼Œå»ç”Ÿæˆ Tensor

        # 4. å®æ—¶è®¡ç®—ç´¢å¼• (æ ¸å¿ƒï¼šå¿…é¡»ç”¨ä¿®æ”¹åçš„ input_path é‡æ–°è®¡ç®— match)
        escaped_target = re.escape(target_name)
        pattern = escaped_target.replace(r'\ ', r'[._\s]+')
        match = re.search(pattern, input_path, re.IGNORECASE)
        
        # å…œåº•ï¼šå¦‚æœéšæœºå¢å¼ºç ´åäº†ç»“æ„å¯¼è‡´åŒ¹é…å¤±è´¥ï¼ˆæå°‘è§ï¼‰ï¼Œå›é€€åˆ°åŸå§‹æ•°æ®
        if not match:
            # print("å¢å¼ºå¯¼è‡´åŒ¹é…å¤±è´¥ï¼Œå›é€€åŸå§‹è·¯å¾„") # è°ƒè¯•ç”¨
            input_path, _ = self.samples[idx]
            match = re.search(pattern, input_path, re.IGNORECASE)

        start_idx = match.start()
        end_idx = match.end()
        
        # 5. è½¬ Tensor å’Œ Padding
        # æˆªæ–­è¾“å…¥ï¼Œé˜²æ­¢å¢å¼ºåé•¿åº¦æº¢å‡º
        input_ids = [self.char_to_idx.get(c.lower(), 1) for c in input_path[:self.max_len]]
        labels = [0.0] * len(input_ids)
        
        limit = min(end_idx, self.max_len)
        for i in range(start_idx, limit):
            labels[i] = 1.0
        
        pad_len = self.max_len - len(input_ids)
        
        # ç¡®ä¿ pad_len ä¸ä¸ºè´Ÿæ•°
        pad_len = max(0, pad_len)
        
        return (
            torch.tensor(input_ids + [0] * pad_len), 
            torch.tensor(labels + [0.0] * pad_len)
        )

# --- è¾…åŠ©å‡½æ•°ï¼šéªŒè¯é›†è®¡ç®— ---
def validate_one_epoch(model, loader, criterion):
    model.eval()
    v_loss = 0
    with torch.no_grad():
        for vx, vy in loader:
            pred = model(vx)
            loss = criterion(pred, vy)
            v_loss += loss.item()
    return v_loss / len(loader) if len(loader) > 0 else 0

# --- è®­ç»ƒé€»è¾‘ ---
def run_train(incremental=False):
    # è®¾ç½®å…¨å±€ç§å­
    set_seed(SEED)
    mode_str = "ã€å¢é‡è®­ç»ƒæ¨¡å¼ã€‘" if incremental else "ã€å…¨é‡è®­ç»ƒæ¨¡å¼ã€‘"
    print(f"{mode_str} éšæœºç§å­å·²å›ºå®šä¸º: {SEED}")

    # 1. æœç´¢æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
    data_files = glob.glob(DATA_FILE_PATTERN)
    # æ’åºä»¥ä¿è¯æ¯æ¬¡è¿è¡Œè¯»å–é¡ºåºä¸€è‡´ï¼Œç¡®ä¿ index 0 æ€»æ˜¯åŒä¸€ä¸ªæ–‡ä»¶
    data_files.sort()
    
    if not data_files:
        print(f"âŒ æœªæ‰¾åˆ°åŒ¹é… {DATA_FILE_PATTERN} çš„æ•°æ®æ–‡ä»¶ã€‚"); return
    
    print(f"å‘ç° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶: {data_files}")

    all_train_lines = []
    all_val_lines = []
    
    # ä½¿ç”¨ç‹¬ç«‹çš„ Random å®ä¾‹è¿›è¡Œ shuffleï¼Œä¸å½±å“å…¨å±€çŠ¶æ€
    rng = random.Random(SEED)
    
    # 2. éå†æ¯ä¸ªæ–‡ä»¶
    for i, f_path in enumerate(data_files):
        with open(f_path, 'r', encoding='utf-8') as f:
            lines = [l.strip() for l in f.readlines() if '#' in l.strip()]
        
        # æ­¥éª¤A: ç¡®å®šæ€§æ‰“ä¹±
        rng.shuffle(lines)
        total_raw = len(lines)
        if total_raw == 0: continue
        
        # æ­¥éª¤B: å…ˆè¿›è¡Œ è®­ç»ƒ/éªŒè¯ åˆ‡åˆ†
        # æ— è®ºæ˜¯å¦å¢é‡ï¼Œæ°¸è¿œå›ºå®šå‰90%ä¸ºè®­ç»ƒæ± ï¼Œå10%ä¸ºéªŒè¯æ± ã€‚
        # è¿™æ ·å¯ä»¥ä¿è¯éªŒè¯é›†æ°¸è¿œçº¯å‡€ï¼Œä¸ä¼šå› ä¸ºå¢é‡è£åˆ‡å¯¼è‡´è®­ç»ƒæ•°æ®è¶Šç•Œã€‚
        split_idx = int(total_raw * 0.9)
        if split_idx == 0 and total_raw > 0: split_idx = total_raw # æå°‘æ•°æ®ä¿æŠ¤
        
        file_train_lines = lines[:split_idx]
        file_val_lines = lines[split_idx:]
        
        # æ­¥éª¤C: å¤„ç†å¢é‡é€»è¾‘ï¼ˆä»…åœ¨åˆ‡åˆ†åçš„å„è‡ªæ± å­å†…è¿›è¡Œä¿ç•™/ä¸¢å¼ƒï¼‰
        if incremental and i == 0:
            # æ—§æ–‡ä»¶ï¼šä»…ä¿ç•™ 2% çš„è®­ç»ƒæ•°æ®ï¼Œä»¥åŠ 2% çš„éªŒè¯æ•°æ® (ä¿æŒåˆ†å¸ƒä¸€è‡´ï¼Œä¸”èŠ‚çœéªŒè¯æ—¶é—´)
            keep_train_count = int(len(file_train_lines) * 0.02)
            keep_val_count = int(len(file_val_lines) * 0.02)
            
            # æœ€å°ä¿ç•™ä¿æŠ¤
            if keep_train_count == 0 and len(file_train_lines) > 0: keep_train_count = 1
            if keep_val_count == 0 and len(file_val_lines) > 0: keep_val_count = 1
            
            final_train = file_train_lines[:keep_train_count]
            final_val = file_val_lines[:keep_val_count]
            
            print(f"  â””â”€ [Old Data] {os.path.basename(f_path)}: é‡‡æ ·ä¿ç•™ è®­ç»ƒ{len(final_train)}æ¡ / éªŒè¯{len(final_val)}æ¡")
        else:
            # æ–°æ–‡ä»¶æˆ–å…¨é‡æ¨¡å¼ï¼šä¿ç•™åˆ‡åˆ†åçš„æ‰€æœ‰æ•°æ®
            final_train = file_train_lines
            final_val = file_val_lines
            print(f"  â””â”€ [New Data] {os.path.basename(f_path)}: å…¨é‡è¯»å– è®­ç»ƒ{len(final_train)}æ¡ / éªŒè¯{len(final_val)}æ¡")

        all_train_lines.extend(final_train)
        all_val_lines.extend(final_val)

    print(f"\næ•°æ®é›†å‡†å¤‡å®Œæ¯•: è®­ç»ƒé›† {len(all_train_lines)} æ¡ | éªŒè¯é›† {len(all_val_lines)} æ¡")

    # 4. æ„å»ºæˆ–åŠ è½½è¯è¡¨
    all_lines_for_vocab = all_train_lines + all_val_lines
    if os.path.exists(VOCAB_PATH):
        with open(VOCAB_PATH, 'rb') as f: char_to_idx = pickle.load(f)
        print("å·²åŠ è½½ç°æœ‰è¯è¡¨ã€‚")
    else:
        # å¼ºè¡Œæ³¨å…¥åŸºç¡€ ASCII å­—ç¬¦ï¼Œé˜²æ­¢è‹±æ–‡è¯è¡¨ç¼ºå¤±
        raw_paths = [l.split('#')[0] for l in all_train_lines + all_val_lines]
        all_chars = set("".join(raw_paths).lower())
        
        ascii_chars = set(string.ascii_lowercase + string.digits + string.punctuation + " ")
        all_chars.update(ascii_chars)
        
        char_to_idx = {c: i+2 for i, c in enumerate(sorted(list(all_chars)))}
        char_to_idx['<PAD>'], char_to_idx['<UNK>'] = 0, 1
        with open(VOCAB_PATH, 'wb') as f: pickle.dump(char_to_idx, f)
        print(f"å·²åˆ›å»ºæ–°è¯è¡¨ï¼ŒåŒ…å« {len(char_to_idx)} ä¸ªå­—ç¬¦ã€‚")

    # 5. åˆ›å»º Dataset å’Œ DataLoader
    train_ds = MovieDataset(all_train_lines, char_to_idx, training=True)
    val_ds = MovieDataset(all_val_lines, char_to_idx, training=False)


    if len(train_ds) < 1:
        print("æœ‰æ•ˆæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒã€‚"); return

    # ã€æ ¸å¿ƒé…ç½®ã€‘ä½¿ç”¨ Generator ç¡®ä¿ shuffle çš„å®Œå…¨å¯å¤ç°æ€§
    g = torch.Generator()
    g.manual_seed(SEED)
    
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, generator=g, num_workers=min(4, NUM_THREADS))
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=min(4, NUM_THREADS))

    # åˆå§‹åŒ–æ–°æ¨¡å‹
    model = Extractor(len(char_to_idx), embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM)
    
    # ä½¿ç”¨ Focal Loss
    criterion = FocalLoss(alpha=0.75, gamma=2)
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5) # å¢åŠ  weight_decay é˜²æ­¢è¿‡æ‹Ÿåˆ
    
    best_val_loss = float('inf')

    # åŠ è½½æ¨¡å‹é€»è¾‘
    if os.path.exists(MODEL_PATH):
        print(f"æ£€æµ‹åˆ°ç°æœ‰æ¨¡å‹ï¼ŒåŠ è½½æƒé‡ä»¥ LR={LR} ç»§ç»­å¾®è°ƒ...")
        model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
        
        if len(val_ds) > 0:
            print("æ­£åœ¨è®¡ç®—å½“å‰æ¨¡å‹çš„åˆå§‹éªŒè¯é›† Loss (åŸºå‡†çº¿)...")
            initial_val_loss = validate_one_epoch(model, val_loader, criterion)
            best_val_loss = initial_val_loss 
            print(f"å½“å‰æ¨¡å‹åŸºå‡† Loss: {best_val_loss:.4f}")
    else:
        print("æœªæ£€æµ‹åˆ°æ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
    
    try:
        for epoch in range(EPOCHS):
            model.train()
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1:02d}")
            for x, y in pbar:
                optimizer.zero_grad()
                pred = model(x)
                loss = criterion(pred, y)
                loss.backward()
                # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
                optimizer.step()
                pbar.set_postfix(loss=f"{loss.item():.4f}")
            
            if len(val_ds) > 0:
                avg_val_loss = validate_one_epoch(model, val_loader, criterion)
                
                if avg_val_loss < best_val_loss:
                    print(f" âœ¨ Loss ä¼˜åŒ– ({best_val_loss:.4f} -> {avg_val_loss:.4f})ï¼Œæ¨¡å‹å·²æ›´æ–°ã€‚")
                    best_val_loss = avg_val_loss
                    torch.save(model.state_dict(), MODEL_PATH)
                else:
                    print(f" â³ éªŒè¯é›† Loss: {avg_val_loss:.4f} (æœªæå‡ï¼Œæœ€ä½³: {best_val_loss:.4f})")
            else:
                torch.save(model.state_dict(), MODEL_PATH)
                print(" âš ï¸ æ— éªŒè¯é›†ï¼Œæ¨¡å‹å·²ä¿å­˜ã€‚")
                
    except KeyboardInterrupt: print("\nğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨åœæ­¢è®­ç»ƒã€‚")

# --- é¢„æµ‹é€»è¾‘ ---
def run_predict(path):
    if not os.path.exists(MODEL_PATH) or not os.path.exists(VOCAB_PATH):
        print("é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æˆ–è¯è¡¨æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œè®­ç»ƒã€‚"); return

    if '#' in path:
        print(path)
        return

    with open(VOCAB_PATH, 'rb') as f: char_to_idx = pickle.load(f)
    model = Extractor(len(char_to_idx), embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM)
    model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
    model.eval()

    # é¢„æµ‹è¾“å…¥è½¬å°å†™
    input_ids = [char_to_idx.get(c.lower(), 1) for c in path[:MAX_LEN]]
    padded = input_ids + [0] * (MAX_LEN - len(input_ids))
    
    with torch.no_grad():
        probs = model(torch.tensor([padded]))[0][:len(path)].numpy()

    # --- åå¤„ç†ç­–ç•¥ ---
    selected_mask = [False] * len(probs)
    
    # 1. é˜ˆå€¼ç­›é€‰
    for i, p in enumerate(probs):
        if p > THRESHOLD: selected_mask[i] = True
            
    # 2. ç©ºæ´å¡«è¡¥ (Gap Filling) - ä¿®å¤ "Iron.Man" ä¸­é—´æ–­å¼€çš„é—®é¢˜
    gap_limit = 2 
    for i in range(len(probs)):
        if selected_mask[i]:
            # å¯»æ‰¾ä¸‹ä¸€ä¸ªè¢«é€‰ä¸­çš„ç‚¹
            for j in range(i + 1, min(i + gap_limit + 2, len(probs))):
                if selected_mask[j]:
                    # å°†ä¸­é—´æ‰€æœ‰éè·¯å¾„åˆ†éš”ç¬¦çš„å­—ç¬¦éƒ½è¿èµ·æ¥
                    for k in range(i + 1, j):
                        if path[k] not in ['/', '\\']:
                            selected_mask[k] = True
                    break

    res_list = []

    if DEBUG_MODE:
        print(f"\n{'='*65}")
        print(f"{'ç´¢å¼•':<4} | {'å­—ç¬¦':<4} | {'åˆ†å€¼':<15} | çŠ¶æ€")
        print("-" * 65)
        for i, p in enumerate(probs):
            status = "âœ… [é€‰ä¸­]" if p > THRESHOLD else "   [æ’é™¤]"
            print(f"{i:<4} | {path[i]:<4} | {p:.10f} | {status}")
        print(f"{'='*65}\n")

    
    for i, is_sel in enumerate(selected_mask):
        if is_sel:
            res_list.append(path[i])

    raw_result = "".join(res_list)
    clean_result = raw_result.replace('.', ' ').replace('_', ' ')
    clean_result = re.sub(r'\s+', ' ', clean_result)
    clean_result = clean_result.strip("/()# â€œâ€.-")

    # 1. éªŒè¯è¿ç»­æ€§
    if clean_result:
        escaped_clean = re.escape(clean_result)
        verify_pattern = escaped_clean.replace(r'\ ', r'[._\s\-\(\)\[\]]*')
        if not re.search(verify_pattern, path, re.IGNORECASE):
            if DEBUG_MODE:
                print(f"[éªŒè¯å¤±è´¥] '{clean_result}' æ— æ³•åœ¨åŸè·¯å¾„ä¸­è¿ç»­åŒ¹é…ï¼Œåˆ¤å®šä¸ºæ— æ•ˆæå–ã€‚")
            clean_result = ""

    # 2. æ··åˆæ¨¡å¼ä¿®å¤
    if clean_result:
        clean_result = TextUtils.fix_name(path, clean_result) 

    if DEBUG_MODE: 
        print(f"æå–åŸæ–‡: {raw_result}")
        print(f"æœ€ç»ˆç»“æœ: {path}#{clean_result}")
    else: 
        print(f"{path}#{clean_result}")

# --- å…¥å£æ§åˆ¶ ---
if __name__ == "__main__":
    if os.path.exists("dbg"):
        DEBUG_MODE = True
        print(f"æ£€æµ‹åˆ° [dbg] æ–‡ä»¶ï¼Œå·²å¼ºåˆ¶å¼€å¯è°ƒè¯•æ¨¡å¼")

    if len(sys.argv) > 1:
        input_arg = sys.argv[1]

        if input_arg == '--inc':
            # æ¨¡å¼ 1: å¢é‡è®­ç»ƒ
            run_train(incremental=True)
        
        elif os.path.exists(input_arg) and os.path.isfile(input_arg):
            # æ¨¡å¼ 2: æ‰¹é‡é¢„æµ‹ (è¾“å…¥æ˜¯æ–‡ä»¶è·¯å¾„)
            try:
                print(f"æ£€æµ‹åˆ°è¾“å…¥ä¸ºæ–‡ä»¶: [{input_arg}]ï¼Œå¼€å§‹æ‰¹é‡å¤„ç†...")
                with open(input_arg, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                total_lines = len(lines)
                for idx, line in enumerate(lines):
                    line = line.strip()
                    if not line: continue
                    run_predict(line)
                    
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        
        else:
            # æ¨¡å¼ 3: å•æ¡å­—ç¬¦ä¸²é¢„æµ‹
            run_predict(input_arg)
    else:
        # æ¨¡å¼ 4: é»˜è®¤å…¨é‡è®­ç»ƒ
        run_train(incremental=False)

