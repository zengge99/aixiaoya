import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import pickle
import sys
import os
import re
import random
import numpy as np
import glob  # å¼•å…¥globç”¨äºåŒ¹é…å¤šä¸ªæ–‡ä»¶

# --- å…¨å±€æ ¸å¿ƒé…ç½® ---
NUM_THREADS = 4
BATCH_SIZE = 64
LR = 1e-4            # å­¦ä¹ ç‡
EPOCHS = 50          # è®­ç»ƒè½®æ•°
MAX_LEN = 150        # æœ€å¤§è·¯å¾„é•¿åº¦
MODEL_PATH = "movie_model.pth"
VOCAB_PATH = "vocab.pkl"
# æ•°æ®æ–‡ä»¶åŒ¹é…æ¨¡å¼ (åŒ¹é… train_data.txt, train_data_2.txt ç­‰)
DATA_FILE_PATTERN = "train_data*.txt" 
SEED = 42            # ğŸ² å›ºå®šéšæœºç§å­

# --- é¢„æµ‹/è°ƒè¯•é…ç½® ---
DEBUG_MODE = True    # å¼€å¯åæ˜¾ç¤ºå…¨è·¯å¾„æ‰€æœ‰å­—ç¬¦å¾—åˆ†
THRESHOLD = 0.2      # æ ¸å¿ƒåˆ¤å®šé˜ˆå€¼
SMOOTH_VAL = 0.05    # è¾…åŠ©åˆ¤å®šé˜ˆå€¼ï¼ˆç”¨äºæ•‘å›ä¸­é—´å­—ç¬¦ï¼‰

# å¿…é¡»åœ¨ import torch ä¹‹åç«‹å³è®¾ç½®
torch.set_num_threads(NUM_THREADS)

# --- ğŸ› ï¸ è¾…åŠ©å‡½æ•°ï¼šå›ºå®šéšæœºç§å­ ---
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    # ä¿è¯cudnnå¯å¤ç°æ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# --- ç§»æ¤çš„ JS é€»è¾‘å·¥å…·ç±» ---
class TextUtils:
    CN_NUMS = ["é›¶", "ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å"]

    @staticmethod
    def number2text(text):
        """
        å°†æ•°å­—å­—ç¬¦ä¸²è½¬æ¢ä¸ºä¸­æ–‡æ•°å­—
        """
        if not text: return text
        text = text.lstrip('0')
        if not text: return "é›¶" 

        try:
            num = int(text)
        except ValueError:
            return text

        if num <= 10:
            return TextUtils.CN_NUMS[num]
        elif num < 20:
            return "å" + TextUtils.CN_NUMS[num % 10]
        elif num % 10 == 0:
            return TextUtils.CN_NUMS[num // 10] + "å"
        else:
            return TextUtils.CN_NUMS[num // 10] + "å" + TextUtils.CN_NUMS[num % 10]

    @staticmethod
    def fix_name(path, ai_result):
        """
        AI ç»“æœåå¤„ç†ï¼šä¿®æ­£æˆ–è¡¥å…¨å­£æ•°ä¿¡æ¯
        """
        replace_patterns = [
            r'Season\s*(\d{1,2})',              
            r'SE(\d{1,2})',                     
            r'(?<![a-zA-Z])S(\d{1,2})(?![a-zA-Z])', 
            r'ç¬¬(\d{1,2})å­£'                    
        ]

        processed_result = ai_result
        replaced_flag = False 

        def replace_func(match):
            nonlocal replaced_flag
            replaced_flag = True
            num = match.group(1)
            cn_num = TextUtils.number2text(num)
            return f" ç¬¬{cn_num}å­£ " 

        # 1. å°è¯•åœ¨ AI ç»“æœå†…éƒ¨ç›´æ¥æ›¿æ¢
        for pattern in replace_patterns:
            if re.search(pattern, processed_result, re.IGNORECASE):
                processed_result = re.sub(pattern, replace_func, processed_result, flags=re.IGNORECASE)
        
        processed_result = re.sub(r'\s+', ' ', processed_result).strip()

        if replaced_flag:
            return processed_result

        # 2. (å…œåº•) ä»åŸè·¯å¾„æ‰¾å­£æ•°è¿½åŠ 
        path_search_patterns = [
            r'Season\s*(\d{1,2})',
            r'SE(\d{1,2})',
            r'ç¬¬(\d{1,2})å­£',
            r'(?<![A-Za-z])S(\d{1,2})'
        ]

        for pattern in path_search_patterns:
            match = re.search(pattern, path, re.IGNORECASE)
            if match:
                num = match.group(1)
                cn_num = TextUtils.number2text(num)
                suffix = f"ç¬¬{cn_num}å­£"
                if suffix not in processed_result:
                    return f"{processed_result} {suffix}"
                break 
        
        return processed_result

# --- æ¨¡å‹ç»“æ„å®šä¹‰ ---
class FilmExtractor(nn.Module):
    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.gru = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        embedded = self.embedding(x)
        gru_out, _ = self.gru(embedded)
        return self.fc(gru_out).squeeze(-1)

# --- æ•°æ®é›†å®šä¹‰ ---
class MovieDataset(Dataset):
    def __init__(self, lines, char_to_idx, max_len=MAX_LEN):
        self.samples = []
        skipped_count = 0
        
        for line in lines:
            line = line.strip()
            if '#' not in line: continue
            input_path, target_name = line.rsplit('#', 1)
            target_name = target_name.strip()
            
            escaped_target = re.escape(target_name)
            pattern = escaped_target.replace(r'\ ', r'[._\s]+')
            match = re.search(pattern, input_path, re.IGNORECASE)
            
            if match:
                start_idx = match.start()
                end_idx = match.end()
                
                input_ids = [char_to_idx.get(c, 1) for c in input_path[:max_len]]
                labels = [0.0] * len(input_ids)
                
                limit = min(end_idx, max_len)
                for i in range(start_idx, limit):
                    labels[i] = 1.0
                
                pad_len = max_len - len(input_ids)
                self.samples.append((
                    torch.tensor(input_ids + [0] * pad_len), 
                    torch.tensor(labels + [0.0] * pad_len)
                ))
            else:
                skipped_count += 1

        if skipped_count > 0:
            print(f"Dataset Info: è·³è¿‡äº† {skipped_count} æ¡æ— æ³•åŒ¹é…æ ‡ç­¾çš„æ•°æ®ã€‚")

    def __len__(self): return len(self.samples)
    def __getitem__(self, idx): return self.samples[idx]

# --- ğŸ› ï¸ è¾…åŠ©å‡½æ•°ï¼šéªŒè¯é›†è®¡ç®— ---
def validate_one_epoch(model, loader, criterion):
    model.eval()
    v_loss = 0
    with torch.no_grad():
        for vx, vy in loader:
            pred = model(vx)
            loss = criterion(pred, vy)
            v_loss += loss.item()
    return v_loss / len(loader) if len(loader) > 0 else 0

# --- è®­ç»ƒé€»è¾‘ ---
def run_train(incremental=False):
    # è®¾ç½®å…¨å±€ç§å­
    set_seed(SEED)
    mode_str = "ã€å¢é‡è®­ç»ƒæ¨¡å¼ã€‘" if incremental else "ã€å…¨é‡è®­ç»ƒæ¨¡å¼ã€‘"
    print(f"{mode_str} éšæœºç§å­å·²å›ºå®šä¸º: {SEED}")

    # 1. æœç´¢æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
    data_files = glob.glob(DATA_FILE_PATTERN)
    # æ’åºä»¥ä¿è¯æ¯æ¬¡è¿è¡Œè¯»å–é¡ºåºä¸€è‡´ï¼Œç¡®ä¿ index 0 æ€»æ˜¯åŒä¸€ä¸ªæ–‡ä»¶
    data_files.sort()
    
    if not data_files:
        print(f"âŒ æœªæ‰¾åˆ°åŒ¹é… {DATA_FILE_PATTERN} çš„æ•°æ®æ–‡ä»¶ã€‚"); return
    
    print(f"å‘ç° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶: {data_files}")

    all_train_lines = []
    all_val_lines = []
    
    # 2. éå†æ¯ä¸ªæ–‡ä»¶
    # ä½¿ç”¨ç‹¬ç«‹çš„ Random å®ä¾‹è¿›è¡Œ shuffleï¼Œä¸å½±å“å…¨å±€çŠ¶æ€
    rng = random.Random(SEED)
    
    for i, f_path in enumerate(data_files):
        with open(f_path, 'r', encoding='utf-8') as f:
            lines = [l.strip() for l in f.readlines() if '#' in l.strip()]
        
        # ç¡®å®šæ€§æ‰“ä¹±
        rng.shuffle(lines)
        
        total_raw = len(lines)
        if total_raw == 0: continue
        
        # --- å¢é‡è®­ç»ƒæ ¸å¿ƒé€»è¾‘ ---
        if incremental and i == 0:
            # å¦‚æœæ˜¯å¢é‡æ¨¡å¼ï¼Œä¸”æ˜¯ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼ˆæ—§æ•°æ®ï¼‰ï¼Œåªä¿ç•™ 10%
            keep_count = int(total_raw * 0.1)
            # è‡³å°‘ä¿ç•™1æ¡ï¼Œé¿å…ç©ºåˆ—è¡¨
            if keep_count == 0 and total_raw > 0: keep_count = 1
            
            lines = lines[:keep_count]
            print(f"  â””â”€ [Old Data] {os.path.basename(f_path)}: ä»…å– 10% ({keep_count}/{total_raw}æ¡)")
        else:
            # å…¶ä»–æƒ…å†µï¼ˆå…¨é‡æ¨¡å¼ æˆ– å¢é‡æ¨¡å¼ä¸‹çš„æ–°æ–‡ä»¶ï¼‰ï¼Œä¿ç•™ 100%
            print(f"  â””â”€ [New Data] {os.path.basename(f_path)}: è¯»å–å…¨é‡ ({total_raw}æ¡)")

        # 3. å¯¹ç­›é€‰åçš„æ•°æ®è¿›è¡Œ è®­ç»ƒ/éªŒè¯ åˆ‡åˆ† (90% / 10%)
        # å³ä½¿æ˜¯ Old Dataï¼Œæˆ‘ä»¬ä¹Ÿåˆ‡åˆ†å‡ºéªŒè¯é›†ï¼Œä»¥ä¿è¯éªŒè¯ Loss çš„æœ‰æ•ˆæ€§
        current_total = len(lines)
        train_count = int(current_total * 0.9)
        if train_count == 0 and current_total > 0: train_count = current_total
        
        train_part = lines[:train_count]
        val_part = lines[train_count:]
        
        all_train_lines.extend(train_part)
        all_val_lines.extend(val_part)

    print(f"\næ•°æ®é›†å‡†å¤‡å®Œæ¯•: è®­ç»ƒé›† {len(all_train_lines)} æ¡ | éªŒè¯é›† {len(all_val_lines)} æ¡")

    # 4. æ„å»ºæˆ–åŠ è½½è¯è¡¨ (åŸºäºæ‰€æœ‰æ•°æ®)
    all_lines_for_vocab = all_train_lines + all_val_lines
    if os.path.exists(VOCAB_PATH):
        with open(VOCAB_PATH, 'rb') as f: char_to_idx = pickle.load(f)
        print("å·²åŠ è½½ç°æœ‰è¯è¡¨ã€‚")
    else:
        # æ³¨æ„ï¼šå¦‚æœæ˜¯å¢é‡è®­ç»ƒä¸”æ²¡æœ‰æ—§è¯è¡¨ï¼Œå¯èƒ½ä¼šæ¼æ‰æ—§æ•°æ®é‡Œè¢«ä¸¢å¼ƒçš„é‚£90%å­—ç¬¦
        # ä½†é€šå¸¸å¢é‡è®­ç»ƒæ„å‘³ç€å·²ç»æœ‰æ¨¡å‹å’Œè¯è¡¨äº†ã€‚
        raw_paths = [l.split('#')[0] for l in all_lines_for_vocab]
        all_chars = set("".join(raw_paths))
        char_to_idx = {c: i+2 for i, c in enumerate(sorted(list(all_chars)))}
        char_to_idx['<PAD>'], char_to_idx['<UNK>'] = 0, 1
        with open(VOCAB_PATH, 'wb') as f: pickle.dump(char_to_idx, f)
        print(f"å·²åˆ›å»ºæ–°è¯è¡¨ï¼ŒåŒ…å« {len(char_to_idx)} ä¸ªå­—ç¬¦ã€‚")

    # 5. åˆ›å»º Dataset å’Œ DataLoader
    train_ds = MovieDataset(all_train_lines, char_to_idx)
    val_ds = MovieDataset(all_val_lines, char_to_idx)

    if len(train_ds) < 1:
        print("æœ‰æ•ˆæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒã€‚"); return

    # è¿™é‡Œä½¿ç”¨äº† generator æ¥ç¡®ä¿ shuffle çš„å®Œå…¨å¯å¤ç°æ€§
    g = torch.Generator()
    g.manual_seed(SEED)
    
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, generator=g)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)

    model = FilmExtractor(len(char_to_idx))
    criterion = nn.BCELoss()
    optimizer = optim.AdamW(model.parameters(), lr=LR)
    
    best_val_loss = float('inf')

    # åŠ è½½æ¨¡å‹é€»è¾‘
    if os.path.exists(MODEL_PATH):
        print(f"æ£€æµ‹åˆ°ç°æœ‰æ¨¡å‹ï¼ŒåŠ è½½æƒé‡ä»¥ LR={LR} ç»§ç»­å¾®è°ƒ...")
        model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
        
        if len(val_ds) > 0:
            print("æ­£åœ¨è®¡ç®—å½“å‰æ¨¡å‹çš„åˆå§‹éªŒè¯é›† Loss (åŸºå‡†çº¿)...")
            initial_val_loss = validate_one_epoch(model, val_loader, criterion)
            best_val_loss = initial_val_loss 
            print(f"å½“å‰æ¨¡å‹åŸºå‡† Loss: {best_val_loss:.4f}")
    else:
        print("ğŸ†• æœªæ£€æµ‹åˆ°æ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
    
    try:
        for epoch in range(EPOCHS):
            model.train()
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1:02d}")
            for x, y in pbar:
                optimizer.zero_grad()
                pred = model(x)
                loss = criterion(pred, y)
                loss.backward()
                optimizer.step()
                pbar.set_postfix(loss=f"{loss.item():.4f}")
            
            if len(val_ds) > 0:
                avg_val_loss = validate_one_epoch(model, val_loader, criterion)
                
                if avg_val_loss < best_val_loss:
                    print(f" âœ¨ Loss ä¼˜åŒ– ({best_val_loss:.4f} -> {avg_val_loss:.4f})ï¼Œæ¨¡å‹å·²æ›´æ–°ã€‚")
                    best_val_loss = avg_val_loss
                    torch.save(model.state_dict(), MODEL_PATH)
                else:
                    print(f" â³ éªŒè¯é›† Loss: {avg_val_loss:.4f} (æœªæå‡ï¼Œæœ€ä½³: {best_val_loss:.4f})")
            else:
                torch.save(model.state_dict(), MODEL_PATH)
                print(" âš ï¸ æ— éªŒè¯é›†ï¼Œæ¨¡å‹å·²ä¿å­˜ã€‚")
                
    except KeyboardInterrupt: print("\nğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨åœæ­¢è®­ç»ƒã€‚")

# --- é¢„æµ‹é€»è¾‘ ---
def run_predict(path):
    if not os.path.exists(MODEL_PATH) or not os.path.exists(VOCAB_PATH):
        print("é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æˆ–è¯è¡¨æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œè®­ç»ƒã€‚"); return

    with open(VOCAB_PATH, 'rb') as f: char_to_idx = pickle.load(f)
    model = FilmExtractor(len(char_to_idx))
    model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
    model.eval()

    input_ids = [char_to_idx.get(c, 1) for c in path[:MAX_LEN]]
    padded = input_ids + [0] * (MAX_LEN - len(input_ids))
    
    with torch.no_grad():
        probs = model(torch.tensor([padded]))[0][:len(path)].numpy()

    if DEBUG_MODE:
        print(f"\n{'='*65}")
        print(f"{'ç´¢å¼•':<4} | {'å­—ç¬¦':<4} | {'åˆ†å€¼':<15} | çŠ¶æ€")
        print("-" * 65)
        for i, p in enumerate(probs):
            status = "âœ… [é€‰ä¸­]" if p > THRESHOLD else "   [æ’é™¤]"
            print(f"{i:<4} | {path[i]:<4} | {p:.10f} | {status}")
        print(f"{'='*65}\n")

    res_list = []
    for i, p in enumerate(probs):
        is_high = p > THRESHOLD
        is_bridge = False
        if not is_high and p > SMOOTH_VAL:
            left_high = probs[i-1] > THRESHOLD if i > 0 else False
            right_high = probs[i+1] > THRESHOLD if i < len(probs)-1 else False
            if left_high and right_high:
                is_bridge = True
        
        if is_high or is_bridge:
            res_list.append(path[i])
    
    raw_result = "".join(res_list)
    clean_result = raw_result.replace('.', ' ').replace('_', ' ')
    clean_result = re.sub(r'\s+', ' ', clean_result)
    clean_result = clean_result.strip("/()# â€œâ€.-")

    # 1. éªŒè¯è¿ç»­æ€§
    if clean_result:
        escaped_clean = re.escape(clean_result)
        verify_pattern = escaped_clean.replace(r'\ ', r'[._\s\-\(\)\[\]]*')
        if not re.search(verify_pattern, path, re.IGNORECASE):
            if DEBUG_MODE:
                print(f"[éªŒè¯å¤±è´¥] '{clean_result}' æ— æ³•åœ¨åŸè·¯å¾„ä¸­è¿ç»­åŒ¹é…ï¼Œåˆ¤å®šä¸ºæ— æ•ˆæå–ã€‚")
            clean_result = ""

    # 2. æ··åˆæ¨¡å¼ä¿®å¤
    if clean_result:
        clean_result = TextUtils.fix_name(path, clean_result) 

    if DEBUG_MODE: 
        print(f"æå–åŸæ–‡: {raw_result}")
        print(f"æœ€ç»ˆç»“æœ: {clean_result}\n")
    else: 
        print(clean_result)

# --- å…¥å£æ§åˆ¶ ---
if __name__ == "__main__":
    # å¦‚æœæœ‰å‚æ•°
    if len(sys.argv) > 1:
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¢é‡è®­ç»ƒæ ‡è®°
        if sys.argv[1] == '--inc':
            run_train(incremental=True)
        else:
            # å¦åˆ™è§†ä¸ºé¢„æµ‹è·¯å¾„
            run_predict(sys.argv[1])
    else:
        # é»˜è®¤å…¨é‡è®­ç»ƒ
        run_train(incremental=False)